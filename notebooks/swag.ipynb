{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "76987774-7d7a-4bd2-a0ac-a9abaf6d5c7e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "#%matplotlib notebook\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c61db724-c905-4458-a58f-d3cab189b8b1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import operator\n",
    "\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(\"../src\") if \"../src\" not in sys.path else None \n",
    "\n",
    "# custom imports\n",
    "from methods import *\n",
    "from nnmodules import GenericDNN\n",
    "from swag import *\n",
    "from evaluation import * "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "705e4fe4-9ab3-410f-b7e4-30637a0e0cf0",
   "metadata": {},
   "source": [
    "### Initialize Data and Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "268c1570-90c6-4c4c-9f65-a5e6e9c7eb93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose dataset to use\n",
    "dataset_name = 'energy_heating_load.txt'#'yacht.txt'\n",
    "datasets_path = '../datasets/'\n",
    "dataset_path = datasets_path + dataset_name\n",
    "\n",
    "# load data and create torch training data loader\n",
    "(X_train, y_train), (X_test, y_test) = load_data(dataset_path)\n",
    "trainloader = torch.utils.data.DataLoader(Dataset(X_train, y_train, dataset_name.split('.')[0]), \n",
    "                                         batch_size:=32, shuffle:=True)\n",
    "\n",
    "# init model, criterion and optimizer\n",
    "net = GenericDNN(input_size:=X_train.shape[-1], hidden_size:=input_size, output_size:=1)\n",
    "criterion = torch.nn.MSELoss()#L1Loss()\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=0.01, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56e73d18-b356-4f58-ab32-1d117a8c67d6",
   "metadata": {},
   "source": [
    "### Choose whether to train weights (SWAG), and do inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6fc5f558-a9be-472e-80d7-94ac3d7b3daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set 'train:=True' if model not previously trained, or want to train a new model\n",
    "checkpoint_path = '../checkpoints/'\n",
    "if train:=False:\n",
    "    # delete previous checkpoints for model with current dataset\n",
    "    onlyfiles = [f for f in os.listdir(checkpoint_path) if os.path.isfile(os.path.join(checkpoint_path, f))]\n",
    "    for file in onlyfiles:\n",
    "        if trainloader.dataset.name in file:\n",
    "            print(f\"Deletes {file=}\")\n",
    "            os.remove(checkpoint_path + file)\n",
    "    \n",
    "    # train and save weights\n",
    "    train_swag(\n",
    "        net,\n",
    "        trainloader,\n",
    "        optimizer,\n",
    "        criterion,\n",
    "        init_epochs=1000,\n",
    "        sampling_epochs=5,\n",
    "        nsamples=200,\n",
    "        path_to_checkpoints=\"../checkpoints/\",\n",
    "    )\n",
    "\n",
    "# Do inference (infer approximate sample mean and covariance of an assumed Guassian posterior)\n",
    "theta_SWA, cov_diag, D = inference_swag(trainloader.dataset.name, checkpoint_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8174e861-f5f3-4206-8be7-478e3e340a53",
   "metadata": {},
   "source": [
    "### Do predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cb53353d-0100-486c-b0f2-5c53ae25b8b0",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "shape '[1]' is invalid for input of size 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-03cd3e98c879>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Do predictions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m Lpi, Upi, mu_swa, mu_pred = monte_carlo_PI(trainloader.dataset[:][0], net, theta_SWA, cov_diag, \n\u001b[0m\u001b[1;32m      3\u001b[0m                                             D, nsamples=50, percentile=0.9)\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# invert prediction normalization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Dropbox/PhD BI-KI/Kurs/ProbAi 2021/project/dt8122-2021/notebooks/../src/swag.py\u001b[0m in \u001b[0;36mmonte_carlo_PI\u001b[0;34m(x, model, theta_SWA, cov_diag, D, nsamples, percentile)\u001b[0m\n\u001b[1;32m    180\u001b[0m         \u001b[0;31m# sample weights and add those weights to the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m         \u001b[0mtweights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample_posterior_swag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtheta_SWA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcov_diag\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mD\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 182\u001b[0;31m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor2ordereddict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtweights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    183\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m         \u001b[0;31m# do prediction with current (wrt. weights) model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Dropbox/PhD BI-KI/Kurs/ProbAi 2021/project/dt8122-2021/notebooks/../src/methods.py\u001b[0m in \u001b[0;36mtensor2ordereddict\u001b[0;34m(tensor, odict)\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32min\u001b[0m \u001b[0modict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[0mtmp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m         \u001b[0modict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mn\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtmp\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m         \u001b[0mn\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mtmp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0modict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: shape '[1]' is invalid for input of size 0"
     ]
    }
   ],
   "source": [
    "# Do predictions\n",
    "Lpi, Upi, mu_swa, mu_pred = monte_carlo_PI(trainloader.dataset[:][0], net, theta_SWA, cov_diag, \n",
    "                                            D, nsamples=50, percentile=0.9)\n",
    "\n",
    "# invert prediction normalization\n",
    "invert_normalization = lambda y: y * trainloader.dataset.stdy + trainloader.dataset.muy\n",
    "Lpi, Upi, mu_swa, mu_pred = map(invert_normalization, (Lpi, Upi, mu_swa, mu_pred))\n",
    "\n",
    "# get non-normalized y_true\n",
    "trainloader.dataset.normalize = False\n",
    "y_true = trainloader.dataset[:][1]\n",
    "trainloader.dataset.normalize = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19e972de-df50-4e6d-9945-81ec78645734",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting\n",
    "fig, ax = plt.subplots(figsize=(14,5))\n",
    "ax.plot(y_true, \"-o\", label=\"$y$\")\n",
    "ax.plot(mu_swa[:,0], \"-o\", label=\"$\\\\hat{y} \\\\mid \\\\theta_{SWA}$\")\n",
    "ax.plot(mu_pred[:,0], \"-o\", label=\"$\\\\hat{y} \\\\mid \\\\bar{\\\\theta}$\")\n",
    "ax.set_xlabel(\"#data\")\n",
    "ax.set_ylabel(\"Response variable\")\n",
    "ax.legend()\n",
    "fig.suptitle(\"True vs Predicted\", size=25)\n",
    "ax.fill_between(np.arange(len(y_train)), Lpi[:,0], Upi[:,0], color='b', alpha=.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a334bbfb-0ef9-4bc3-b844-9b78418c6efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(y_true, mu_swa[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a38ed855-1a65-4173-8365-69e718fd0844",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate model\n",
    "print(f\"{rmse(y_true, mu_swa)=}, {picp(Lpi, Upi, y_true)=}, {mpiw(Lpi, Upi)=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc65d529-d1de-4fda-b5ce-5ac4974d9ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot uncertainty curve and PI difference\n",
    "# - We expect the PI to be wider where there is few data points, i.e. at the peaks of the \"yatch.txt\" \n",
    "# dataset, but the plot above does not seem to support this fact. However, when the graph \"grows\"\n",
    "# quickly, the PI width appears thin, while it in reality is actually wider. \n",
    "# This can be seen from the follow plot: I.e. that the PI difference grows near the peaks, and\n",
    "# becomes smaller near the troughs.\n",
    "fig, ax = plt.subplots(figsize=(14,5))\n",
    "ax.plot(Upi[:,0] - Lpi[:,0], label='PI-difference')\n",
    "ax.fill_between(np.arange(len(y_train)), Lpi[:,0], Upi[:,0], color='b', alpha=.1)\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2703bde2-40cb-4bac-b504-fc0d3d81791d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
